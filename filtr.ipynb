{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RAC0001\\Anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:21: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import time\n",
    "import helper_pol as h\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "#load_dotenv('/home/ca-polandsys/.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_code = 'PL'\n",
    "\n",
    "h.connect_to_rdb()\n",
    "query = r\"SELECT min(max_report_date) AS max_report_date FROM stats.vw_partner_consumer_data_loads_by_country WHERE country_code = '\"+country_code+\"' AND fact_table IN ('spotify.fact_streams','spotify.fact_streams_orchard')\"\n",
    "spotify_max_date_df = h.query_sql(query)\n",
    "\n",
    "spotify_max_date = datetime.today()\n",
    "\n",
    "try:\n",
    "    spotify_max_date = spotify_max_date_df['max_report_date'].loc[0]\n",
    "\n",
    "except:\n",
    "    next\n",
    "    \n",
    "\n",
    "query = r\"SELECT min(max_report_date) AS max_report_date FROM stats.vw_partner_consumer_data_loads_by_country WHERE country_code = '\"+country_code+\"' AND fact_table IN ('tiktok.fact_views')\"\n",
    "tiktok_max_date_df = h.query_sql(query)\n",
    "\n",
    "tiktok_max_date = datetime.today()\n",
    "\n",
    "try:\n",
    "    tiktok_max_date = tiktok_max_date_df['max_report_date'].loc[0]\n",
    "\n",
    "except:\n",
    "    next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sql/tracks_sme.sql\", \"r\") as file:\n",
    "    tracks_sme_sql = file.read()\n",
    "    \n",
    "    \n",
    "tracks_sme_sql = tracks_sme_sql.replace('2024-01-07',spotify_max_date.strftime(\"%Y-%m-%d\"))\n",
    "tracks_sme_sql = tracks_sme_sql.replace('_COUNTRY_CODE_',country_code)\n",
    "tracks_sme_df = h.query_sql(tracks_sme_sql)\n",
    "tracks_sme_df['feed'] = 'SME'\n",
    "\n",
    "\n",
    "with open(\"sql/tracks_orchard.sql\", \"r\") as file:\n",
    "    tracks_orch_sql = file.read()\n",
    "\n",
    "tracks_orch_sql = tracks_orch_sql.replace('2024-01-07',spotify_max_date.strftime(\"%Y-%m-%d\"))\n",
    "tracks_orch_sql = tracks_orch_sql.replace('_COUNTRY_CODE_',country_code)\n",
    "tracks_orch_df = h.query_sql(tracks_orch_sql)\n",
    "tracks_orch_df['feed'] = 'ORCHARD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sql/tiktok_sony.sql\", \"r\") as file:\n",
    "    tiktok_sme_sql = file.read()\n",
    "    \n",
    "    \n",
    "tiktok_sme_sql = tiktok_sme_sql.replace('2024-01-07',tiktok_max_date.strftime(\"%Y-%m-%d\"))\n",
    "tiktok_sme_sql = tiktok_sme_sql.replace('_COUNTRY_CODE_',country_code)\n",
    "tiktok_sme_df = h.query_sql(tiktok_sme_sql)\n",
    "tiktok_sme_df['feed'] = 'SME'\n",
    "\n",
    "with open(\"sql/tiktok_orchard.sql\", \"r\") as file:\n",
    "    tiktok_orch_sql = file.read()\n",
    "    \n",
    "    \n",
    "tiktok_orch_sql = tiktok_orch_sql.replace('2024-01-07',tiktok_max_date.strftime(\"%Y-%m-%d\"))\n",
    "tiktok_orch_sql = tiktok_orch_sql.replace('_COUNTRY_CODE_',country_code)\n",
    "tiktok_orch_df = h.query_sql(tiktok_orch_sql)\n",
    "tiktok_orch_df['feed'] = 'ORCHARD'\n",
    "\n",
    "tiktok_df = pd.concat([tiktok_sme_df, tiktok_orch_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_df = pd.concat([tracks_sme_df, tracks_orch_df], ignore_index=True)\n",
    "\n",
    "df_metadata = tracks_df.drop_duplicates(subset='track_isrc')\n",
    "df_metadata = df_metadata.drop(columns=['segment_name','segment_streams'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_isrcs = ','.join([\"'{}'\".format(isrc) for isrc in df_metadata['track_isrc']])\n",
    "\n",
    "with open(\"sql/metadane_release_date.sql\", \"r\") as file:\n",
    "    metadane_release_date_sql = file.read()\n",
    "\n",
    "metadane_release_date_sql = metadane_release_date_sql.replace('_ISRC_CD_',combined_isrcs)\n",
    "metadane_release_date = h.query_sql(metadane_release_date_sql)\n",
    "metadane_release_date['release_date'] = pd.to_datetime(metadane_release_date['release_date'])\n",
    "\n",
    "min_release_dates_df = metadane_release_date.groupby('track_isrc')['release_date'].min().reset_index()\n",
    "min_release_dates_df = pd.DataFrame(min_release_dates_df, columns=['track_isrc', 'release_date'])\n",
    "\n",
    "df_metadata = pd.merge(df_metadata, min_release_dates_df, on='track_isrc', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_df = pd.pivot_table(tracks_df, values='segment_streams', index=['track_isrc'],\n",
    "                          columns='segment_name', aggfunc='sum', fill_value=0)\n",
    "genres_df = pivot_df.div(pivot_df.sum(axis=1), axis=0) * 100\n",
    "genres_df.reset_index(inplace=True)\n",
    "\n",
    "none_df = (tracks_df.groupby('track_isrc')\n",
    "                       .apply(lambda x: x.loc[x['segment_name'].isna(), 'segment_streams'].sum() / x['segment_streams'].sum())\n",
    "                       .reset_index(name='share_of_none_values'))\n",
    "\n",
    "genres_df = pd.merge(genres_df, none_df, on='track_isrc', how='outer')\n",
    "\n",
    "\n",
    "genres_df = pd.merge(df_metadata, genres_df, on='track_isrc', how='inner')\n",
    "genres_df = genres_df.sort_values(by='total_streams', ascending=False)\n",
    "genres_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tracks removed from the playlist.\n",
      "All tracks removed from the playlist.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Top Hity – Top150 SME+O, ALL\n",
    "\n",
    "playlist_id = '7Le3oCnCpgDO0etNcbyTeM'\n",
    "h.refresh_spotify_conn()\n",
    "\n",
    "df_playlist = genres_df[0:150]\n",
    "\n",
    "if len(df_playlist) >= 0:\n",
    "    h.spotify_remove_all_tracks_from_playlist(playlist_id)\n",
    "    h.spotify_update_playlist_description(playlist_id,'Dane na dzien '+spotify_max_date.strftime(\"%Y-%m-%d\"))\n",
    "    group_size = 100\n",
    "    track_uri_groups = [df_playlist['_track_uri'].iloc[i:i+group_size].to_list() for i in range(0, len(df_playlist), group_size)]\n",
    "\n",
    "    for i in range(len(track_uri_groups)-1, -1, -1):\n",
    "        track_uri = track_uri_groups[i]\n",
    "        h.spotify_add_tracks(playlist_id,track_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tracks removed from the playlist.\n",
      "All tracks removed from the playlist.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Top Hity – Top150 SME+O, ALL\n",
    "\n",
    "playlist_id = '7Le3oCnCpgDO0etNcbyTeM'\n",
    "\n",
    "df_playlist = genres_df[0:150]\n",
    "\n",
    "if len(df_playlist) >= 0:\n",
    "    h.spotify_remove_all_tracks_from_playlist(playlist_id)\n",
    "    h.spotify_update_playlist_description(playlist_id,'Dane na dzien '+spotify_max_date.strftime(\"%Y-%m-%d\"))\n",
    "    group_size = 100\n",
    "    track_uri_groups = [df_playlist['_track_uri'].iloc[i:i+group_size].to_list() for i in range(0, len(df_playlist), group_size)]\n",
    "\n",
    "    for i in range(len(track_uri_groups)-1, -1, -1):\n",
    "        track_uri = track_uri_groups[i]\n",
    "        h.spotify_add_tracks(playlist_id,track_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tracks removed from the playlist.\n",
      "All tracks removed from the playlist.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Rap hity – Top150 SME+O, ALL\n",
    "\n",
    "playlist_id = '5QqmWfetEZ7IfMvyJycezG'\n",
    "\n",
    "df_playlist = genres_df[(genres_df['rap']>75.0) | genres_df['_track_uri'].str.contains('rap|hip-hop', case=False, regex=True)][0:150]\n",
    "\n",
    "if len(df_playlist) >= 0:\n",
    "    h.spotify_remove_all_tracks_from_playlist(playlist_id)\n",
    "    h.spotify_update_playlist_description(playlist_id,'Dane na dzien '+spotify_max_date.strftime(\"%Y-%m-%d\"))\n",
    "    group_size = 100\n",
    "    track_uri_groups = [df_playlist['_track_uri'].iloc[i:i+group_size].to_list() for i in range(0, len(df_playlist), group_size)]\n",
    "\n",
    "    for i in range(len(track_uri_groups)-1, -1, -1):\n",
    "        track_uri = track_uri_groups[i]\n",
    "        h.spotify_add_tracks(playlist_id,track_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tracks removed from the playlist.\n",
      "All tracks removed from the playlist.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Gram w grę – Top150 SME+O, ALL\n",
    "\n",
    "playlist_id = '4HIyk5bEmoPJVaXwhN3oJu'\n",
    "\n",
    "df_playlist = genres_df[(genres_df['edm']>25.0) | ((genres_df['rap']>55.0) & (genres_df['edm']>1.0)) | genres_df['_track_uri'].str.contains('rap|hip-hop|electro|edm', case=False, regex=True)][0:150]\n",
    "\n",
    "if len(df_playlist) >= 0:\n",
    "    h.spotify_remove_all_tracks_from_playlist(playlist_id)\n",
    "    h.spotify_update_playlist_description(playlist_id,'Dane na dzien '+spotify_max_date.strftime(\"%Y-%m-%d\"))\n",
    "    group_size = 100\n",
    "    track_uri_groups = [df_playlist['_track_uri'].iloc[i:i+group_size].to_list() for i in range(0, len(df_playlist), group_size)]\n",
    "\n",
    "    for i in range(len(track_uri_groups)-1, -1, -1):\n",
    "        track_uri = track_uri_groups[i]\n",
    "        h.spotify_add_tracks(playlist_id,track_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tracks removed from the playlist.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#PPP – Top150 SME+O, ALL\n",
    "\n",
    "playlist_id = '0Tt1VCYMzvK0KtKySr18So'\n",
    "h.refresh_spotify_conn()\n",
    "\n",
    "df_playlist = genres_df[(genres_df['edm']>20.0) | (genres_df['_track_uri'].str.contains('electro|edm', case=False, regex=True))][0:150]\n",
    "\n",
    "if len(df_playlist) >= 0:\n",
    "    h.spotify_remove_all_tracks_from_playlist(playlist_id)\n",
    "    h.spotify_update_playlist_description(playlist_id,'Dane na dzien '+spotify_max_date.strftime(\"%Y-%m-%d\"))\n",
    "    group_size = 100\n",
    "    track_uri_groups = [df_playlist['_track_uri'].iloc[i:i+group_size].to_list() for i in range(0, len(df_playlist), group_size)]\n",
    "\n",
    "    for i in range(len(track_uri_groups)-1, -1, -1):\n",
    "        track_uri = track_uri_groups[i]\n",
    "        h.spotify_add_tracks(playlist_id,track_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tracks removed from the playlist.\n",
      "All tracks removed from the playlist.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Top Hity – Top150 SME+O, ALL\n",
    "\n",
    "playlist_id = '5wGiR9VtUNlfSGaqa7Wniw'\n",
    "\n",
    "cutoff_date = datetime.now() - timedelta(days=56)\n",
    "df_playlist = genres_df[genres_df['release_date'] <= cutoff_date][0:150]\n",
    "\n",
    "if len(df_playlist) >= 0:\n",
    "    h.spotify_remove_all_tracks_from_playlist(playlist_id)\n",
    "    h.spotify_update_playlist_description(playlist_id,'Dane na dzien '+spotify_max_date.strftime(\"%Y-%m-%d\"))\n",
    "    group_size = 100\n",
    "    track_uri_groups = [df_playlist['_track_uri'].iloc[i:i+group_size].to_list() for i in range(0, len(df_playlist), group_size)]\n",
    "\n",
    "    for i in range(len(track_uri_groups)-1, -1, -1):\n",
    "        track_uri = track_uri_groups[i]\n",
    "        h.spotify_add_tracks(playlist_id,track_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from math import sqrt\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return text\n",
    "\n",
    "def calculate_cosine_similarity(str1, str2):\n",
    "    str1 = preprocess_text(str1)\n",
    "    str2 = preprocess_text(str2)\n",
    "\n",
    "    # Create a set of unique characters\n",
    "    all_chars = set(str1 + str2)\n",
    "\n",
    "    # Calculate character frequency (CF) vectors\n",
    "    cf_vector1 = [str1.count(char) for char in all_chars]\n",
    "    cf_vector2 = [str2.count(char) for char in all_chars]\n",
    "\n",
    "    # Calculate the dot product of the CF vectors\n",
    "    dot_product = sum(cf1 * cf2 for cf1, cf2 in zip(cf_vector1, cf_vector2))\n",
    "\n",
    "    # Calculate the magnitude of each CF vector\n",
    "    magnitude1 = sqrt(sum(cf**2 for cf in cf_vector1))\n",
    "    magnitude2 = sqrt(sum(cf**2 for cf in cf_vector2))\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity = dot_product / (magnitude1 * magnitude2) if magnitude1 > 0 and magnitude2 > 0 else 0.0\n",
    "\n",
    "    return similarity\n",
    "\n",
    "def fuzzy_match(row, threshold_name, threshold_artist):\n",
    "    similarity_name = calculate_cosine_similarity(row['_track_name'], row['_track_name_tt'])\n",
    "    similarity_artist = calculate_cosine_similarity(row['_track_artist'], row['_artist_name_tt'])\n",
    "\n",
    "    return (similarity_name >= threshold_name and similarity_artist >= threshold_artist) or (row['track_isrc']==row['isrc_cd'])\n",
    "\n",
    "threshold_name = 0.9  # Example threshold for _track_name\n",
    "threshold_artist = 0.9  # Example threshold for _track_artist\n",
    "\n",
    "cartesian_df = pd.merge(genres_df[0:500].assign(key=1), tiktok_df.assign(key=1), on='key').drop('key', axis=1)\n",
    "\n",
    "matched_pairs = cartesian_df[cartesian_df.apply(lambda row: fuzzy_match(row, threshold_name, threshold_artist), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tracks removed from the playlist.\n",
      "All tracks removed from the playlist.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TOP VIRAL\n",
    "\n",
    "playlist_id = '1Ubg6u4Z7zRRvqrjG1dMnw'\n",
    "\n",
    "df_no_duplicates = matched_pairs.drop_duplicates(subset='_track_uri')\n",
    "\n",
    "df_playlist = df_no_duplicates.sort_values(by='tt_creations', ascending=False)[0:150]\n",
    "\n",
    "\n",
    "if len(df_playlist) >= 0:\n",
    "    h.spotify_remove_all_tracks_from_playlist(playlist_id)\n",
    "    h.spotify_update_playlist_description(playlist_id,'Dane na dzien '+spotify_max_date.strftime(\"%Y-%m-%d\"))\n",
    "    group_size = 100\n",
    "    track_uri_groups = [df_playlist['_track_uri'].iloc[i:i+group_size].to_list() for i in range(0, len(df_playlist), group_size)]\n",
    "\n",
    "    for i in range(len(track_uri_groups)-1, -1, -1):\n",
    "        track_uri = track_uri_groups[i]\n",
    "        h.spotify_add_tracks(playlist_id,track_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tracks removed from the playlist.\n",
      "All tracks removed from the playlist.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_code = 'CZ'\n",
    "\n",
    "h.connect_to_rdb()\n",
    "query = r\"SELECT min(max_report_date) AS max_report_date FROM stats.vw_partner_consumer_data_loads_by_country WHERE country_code = '\"+country_code+\"' AND fact_table IN ('spotify.fact_streams','spotify.fact_streams_orchard')\"\n",
    "spotify_max_date_df = h.query_sql(query)\n",
    "\n",
    "spotify_max_date = datetime.today()\n",
    "\n",
    "try:\n",
    "    spotify_max_date = spotify_max_date_df['max_report_date'].loc[0]\n",
    "\n",
    "except:\n",
    "    next\n",
    "\n",
    "with open(\"sql/tracks_sme.sql\", \"r\") as file:\n",
    "    tracks_sme_sql = file.read()\n",
    "    \n",
    "    \n",
    "tracks_sme_sql = tracks_sme_sql.replace('2024-01-07',spotify_max_date.strftime(\"%Y-%m-%d\"))\n",
    "tracks_sme_sql = tracks_sme_sql.replace('_COUNTRY_CODE_',country_code)\n",
    "tracks_sme_df = h.query_sql(tracks_sme_sql)\n",
    "tracks_sme_df['feed'] = 'SME'\n",
    "\n",
    "\n",
    "with open(\"sql/tracks_orchard.sql\", \"r\") as file:\n",
    "    tracks_orch_sql = file.read()\n",
    "\n",
    "tracks_orch_sql = tracks_orch_sql.replace('2024-01-07',spotify_max_date.strftime(\"%Y-%m-%d\"))\n",
    "tracks_orch_sql = tracks_orch_sql.replace('_COUNTRY_CODE_',country_code)\n",
    "tracks_orch_df = h.query_sql(tracks_orch_sql)\n",
    "tracks_orch_df['feed'] = 'ORCHARD'\n",
    "\n",
    "\n",
    "tracks_df = pd.concat([tracks_sme_df, tracks_orch_df], ignore_index=True)\n",
    "\n",
    "df_metadata = tracks_df.drop_duplicates(subset='track_isrc')\n",
    "df_metadata = df_metadata.drop(columns=['segment_name','segment_streams'])\n",
    "\n",
    "\n",
    "\n",
    "combined_isrcs = ','.join([\"'{}'\".format(isrc) for isrc in df_metadata['track_isrc']])\n",
    "\n",
    "with open(\"sql/metadane_release_date.sql\", \"r\") as file:\n",
    "    metadane_release_date_sql = file.read()\n",
    "\n",
    "metadane_release_date_sql = metadane_release_date_sql.replace('_ISRC_CD_',combined_isrcs)\n",
    "metadane_release_date = h.query_sql(metadane_release_date_sql)\n",
    "metadane_release_date['release_date'] = pd.to_datetime(metadane_release_date['release_date'])\n",
    "\n",
    "min_release_dates_df = metadane_release_date.groupby('track_isrc')['release_date'].min().reset_index()\n",
    "min_release_dates_df = pd.DataFrame(min_release_dates_df, columns=['track_isrc', 'release_date'])\n",
    "\n",
    "df_metadata = pd.merge(df_metadata, min_release_dates_df, on='track_isrc', how='left')\n",
    "\n",
    "\n",
    "pivot_df = pd.pivot_table(tracks_df, values='segment_streams', index=['track_isrc'],\n",
    "                          columns='segment_name', aggfunc='sum', fill_value=0)\n",
    "genres_df = pivot_df.div(pivot_df.sum(axis=1), axis=0) * 100\n",
    "genres_df.reset_index(inplace=True)\n",
    "\n",
    "none_df = (tracks_df.groupby('track_isrc')\n",
    "                       .apply(lambda x: x.loc[x['segment_name'].isna(), 'segment_streams'].sum() / x['segment_streams'].sum())\n",
    "                       .reset_index(name='share_of_none_values'))\n",
    "\n",
    "genres_df = pd.merge(genres_df, none_df, on='track_isrc', how='outer')\n",
    "\n",
    "\n",
    "genres_df = pd.merge(df_metadata, genres_df, on='track_isrc', how='inner')\n",
    "genres_df = genres_df.sort_values(by='total_streams', ascending=False)\n",
    "genres_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#Top Hity – Top150 SME+O, ALL\n",
    "\n",
    "playlist_id = '1e3G6ZlcQrqjoomu3sGZ0m'\n",
    "\n",
    "df_playlist = genres_df[0:150]\n",
    "\n",
    "if len(df_playlist) >= 0:\n",
    "    h.spotify_remove_all_tracks_from_playlist(playlist_id)\n",
    "    h.spotify_update_playlist_description(playlist_id,'Dane na dzien '+spotify_max_date.strftime(\"%Y-%m-%d\"))\n",
    "    group_size = 100\n",
    "    track_uri_groups = [df_playlist['_track_uri'].iloc[i:i+group_size].to_list() for i in range(0, len(df_playlist), group_size)]\n",
    "\n",
    "    for i in range(len(track_uri_groups)-1, -1, -1):\n",
    "        track_uri = track_uri_groups[i]\n",
    "        h.spotify_add_tracks(playlist_id,track_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tracks removed from the playlist.\n",
      "All tracks removed from the playlist.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_code = 'SK'\n",
    "\n",
    "h.connect_to_rdb()\n",
    "query = r\"SELECT min(max_report_date) AS max_report_date FROM stats.vw_partner_consumer_data_loads_by_country WHERE country_code = '\"+country_code+\"' AND fact_table IN ('spotify.fact_streams','spotify.fact_streams_orchard')\"\n",
    "spotify_max_date_df = h.query_sql(query)\n",
    "\n",
    "spotify_max_date = datetime.today()\n",
    "\n",
    "try:\n",
    "    spotify_max_date = spotify_max_date_df['max_report_date'].loc[0]\n",
    "\n",
    "except:\n",
    "    next\n",
    "\n",
    "with open(\"sql/tracks_sme.sql\", \"r\") as file:\n",
    "    tracks_sme_sql = file.read()\n",
    "    \n",
    "    \n",
    "tracks_sme_sql = tracks_sme_sql.replace('2024-01-07',spotify_max_date.strftime(\"%Y-%m-%d\"))\n",
    "tracks_sme_sql = tracks_sme_sql.replace('_COUNTRY_CODE_',country_code)\n",
    "tracks_sme_df = h.query_sql(tracks_sme_sql)\n",
    "tracks_sme_df['feed'] = 'SME'\n",
    "\n",
    "\n",
    "with open(\"sql/tracks_orchard.sql\", \"r\") as file:\n",
    "    tracks_orch_sql = file.read()\n",
    "\n",
    "tracks_orch_sql = tracks_orch_sql.replace('2024-01-07',spotify_max_date.strftime(\"%Y-%m-%d\"))\n",
    "tracks_orch_sql = tracks_orch_sql.replace('_COUNTRY_CODE_',country_code)\n",
    "tracks_orch_df = h.query_sql(tracks_orch_sql)\n",
    "tracks_orch_df['feed'] = 'ORCHARD'\n",
    "\n",
    "\n",
    "tracks_df = pd.concat([tracks_sme_df, tracks_orch_df], ignore_index=True)\n",
    "\n",
    "df_metadata = tracks_df.drop_duplicates(subset='track_isrc')\n",
    "df_metadata = df_metadata.drop(columns=['segment_name','segment_streams'])\n",
    "\n",
    "\n",
    "\n",
    "combined_isrcs = ','.join([\"'{}'\".format(isrc) for isrc in df_metadata['track_isrc']])\n",
    "\n",
    "with open(\"sql/metadane_release_date.sql\", \"r\") as file:\n",
    "    metadane_release_date_sql = file.read()\n",
    "\n",
    "metadane_release_date_sql = metadane_release_date_sql.replace('_ISRC_CD_',combined_isrcs)\n",
    "metadane_release_date = h.query_sql(metadane_release_date_sql)\n",
    "metadane_release_date['release_date'] = pd.to_datetime(metadane_release_date['release_date'])\n",
    "\n",
    "min_release_dates_df = metadane_release_date.groupby('track_isrc')['release_date'].min().reset_index()\n",
    "min_release_dates_df = pd.DataFrame(min_release_dates_df, columns=['track_isrc', 'release_date'])\n",
    "\n",
    "df_metadata = pd.merge(df_metadata, min_release_dates_df, on='track_isrc', how='left')\n",
    "\n",
    "\n",
    "pivot_df = pd.pivot_table(tracks_df, values='segment_streams', index=['track_isrc'],\n",
    "                          columns='segment_name', aggfunc='sum', fill_value=0)\n",
    "genres_df = pivot_df.div(pivot_df.sum(axis=1), axis=0) * 100\n",
    "genres_df.reset_index(inplace=True)\n",
    "\n",
    "none_df = (tracks_df.groupby('track_isrc')\n",
    "                       .apply(lambda x: x.loc[x['segment_name'].isna(), 'segment_streams'].sum() / x['segment_streams'].sum())\n",
    "                       .reset_index(name='share_of_none_values'))\n",
    "\n",
    "genres_df = pd.merge(genres_df, none_df, on='track_isrc', how='outer')\n",
    "\n",
    "\n",
    "genres_df = pd.merge(df_metadata, genres_df, on='track_isrc', how='inner')\n",
    "genres_df = genres_df.sort_values(by='total_streams', ascending=False)\n",
    "genres_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#Top Hity – Top150 SME+O, ALL\n",
    "\n",
    "playlist_id = '5kabOALXCjHfEps1U6dbPv'\n",
    "\n",
    "df_playlist = genres_df[0:150]\n",
    "\n",
    "if len(df_playlist) >= 0:\n",
    "    h.spotify_remove_all_tracks_from_playlist(playlist_id)\n",
    "    h.spotify_update_playlist_description(playlist_id,'Dane na dzien '+spotify_max_date.strftime(\"%Y-%m-%d\"))\n",
    "    group_size = 100\n",
    "    track_uri_groups = [df_playlist['_track_uri'].iloc[i:i+group_size].to_list() for i in range(0, len(df_playlist), group_size)]\n",
    "\n",
    "    for i in range(len(track_uri_groups)-1, -1, -1):\n",
    "        track_uri = track_uri_groups[i]\n",
    "        h.spotify_add_tracks(playlist_id,track_uri)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
